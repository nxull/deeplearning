{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "X.append([1,0,0,0])\n",
    "X.append([0,1,0,0])\n",
    "X.append([0,0,1,0])\n",
    "X.append([0,0,0,1])\n",
    "X.append([0,0,0,1])\n",
    "X.append([1,0,0,0])\n",
    "X.append([0,1,0,0])\n",
    "X.append([0,0,1,0])\n",
    "X.append([0,0,0,1])\n",
    "\n",
    "y = [0.2,0.3,0.4,0.5,0.05,0.1,0.2,0.3,0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def sigmoid_der(x):\n",
    "    return 1.0 - x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "# 4 input variables, 16 hidden units and 1 output variable\n",
    "n_units = (4,16,1)\n",
    "n_layers = len(n_units)\n",
    "\n",
    "layers.append(np.ones(n_units[0]+1+n_units[1]))\n",
    "for i in range(1,n_layers):\n",
    "    layers.append(np.ones(n_units[i]))\n",
    "weights = []\n",
    "for i in range(n_layers-1):\n",
    "    weights.append(np.zeros((layers[i].size,layers[i+1].size)))\n",
    "weights_delta = [0,]*len(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(data):\n",
    "    layers[0][:n_units[0]] = data\n",
    "    layers[0][n_units[0]:-1] = layers[1]\n",
    "    \n",
    "    # propagate the data forwards\n",
    "    for i in range(1,n_layers):\n",
    "        layers[i][...] = sigmoid(np.dot(layers[i-1],weights[i-1]))\n",
    "    return layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(target,update=False,learning_rate=0.1,momentum=0.1):\n",
    "    deltas = []\n",
    "    error = target - layers[-1]\n",
    "    if update is False : return error\n",
    "    delta = error * sigmoid_der(layers[-1])\n",
    "    deltas.append(delta)\n",
    "    \n",
    "    # determine error in hidden layers\n",
    "    for i in range(n_layers-2,0,-1):\n",
    "        delta = np.dot(deltas[0],weights[i].T) * sigmoid_der(layers[i])\n",
    "        deltas.insert(0,delta)\n",
    "    #update weights\n",
    "    for i in range(len(weights)):\n",
    "        layer = np.atleast_2d(layers[i])\n",
    "        delta = np.atleast_2d(deltas[i])\n",
    "        weights_delta_temp = np.dot(layer.T,delta)\n",
    "        weights[i] += learning_rate * weights_delta_temp + momentum * weights_delta[i]\n",
    "        weights_delta[i] = weights_delta_temp\n",
    "    \n",
    "    return (error**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 - loss:0.3116\n",
      "epoch 1000 - loss:0.1660\n",
      "epoch 2000 - loss:0.1801\n",
      "epoch 3000 - loss:0.1877\n",
      "epoch 4000 - loss:0.1914\n",
      "epoch 5000 - loss:0.1922\n",
      "epoch 6000 - loss:0.1920\n",
      "epoch 7000 - loss:0.1916\n",
      "epoch 8000 - loss:0.1913\n",
      "epoch 9000 - loss:0.1912\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10000\n",
    "for i in range(n_epochs):\n",
    "    loss = 0\n",
    "    for j in range(len(X)):\n",
    "        forward(X[j])\n",
    "        backward(y[j])\n",
    "        loss += (y[j] - forward(X[j]))**2\n",
    "    if i%1000 == 0:\n",
    "        print(\"epoch {} - loss:{:04.4f}\".format(i,loss[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [1, 0, 0, 0]; y: 0.20; pred: 0.14\n",
      "X: [0, 1, 0, 0]; y: 0.30; pred: 0.40\n",
      "X: [0, 0, 1, 0]; y: 0.40; pred: 0.29\n",
      "X: [0, 0, 0, 1]; y: 0.50; pred: 0.34\n",
      "X: [0, 0, 0, 1]; y: 0.05; pred: 0.30\n",
      "X: [1, 0, 0, 0]; y: 0.10; pred: 0.16\n",
      "X: [0, 1, 0, 0]; y: 0.20; pred: 0.38\n",
      "X: [0, 0, 1, 0]; y: 0.30; pred: 0.30\n",
      "X: [0, 0, 0, 1]; y: 0.40; pred: 0.33\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X)):\n",
    "    pred = forward(X[i])\n",
    "    loss = backward(y[j],update=False)\n",
    "    print(\"X: {}; y: {:04.2f}; pred: {:04.2f}\".format(X[i],y[i],pred[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation\n",
    "from keras.layers import Embedding,LSTM\n",
    "\n",
    "from keras.datasets import imdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 3s 0us/step\n",
      "Train seq : 25000 \n",
      "Test seq:25000\n"
     ]
    }
   ],
   "source": [
    "n_words = 1000\n",
    "(X_train, Y_train) , (X_test, Y_test) = imdb.load_data(num_words=n_words)\n",
    "print(\"Train seq : {} \\nTest seq:{}\".format(len(X_train),len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train example : [1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32] \n",
      "Test example:[1, 591, 202, 14, 31, 6, 717, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 2, 394, 354, 4, 123, 9, 2, 2, 2, 10, 10, 13, 92, 124, 89, 488, 2, 100, 28, 2, 14, 31, 23, 27, 2, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 2, 451, 202, 14, 6, 717]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train example : {} \\nTest example:{}\".format(X_train[0],X_test[0]))\n",
    "# data is already preprocessed (words are mapped to vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences with max_len\n",
    "max_len = 200\n",
    "X_train = sequence.pad_sequences(X_train,maxlen=max_len)\n",
    "X_test = sequence.pad_sequences(X_test,maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 50)           50000     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200, 50)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               25250     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 135,901\n",
      "Trainable params: 135,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define network architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_words,50,input_length=max_len))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100,dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(Dense(250,activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adagrad\",metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 127s 5ms/step - loss: 0.7014 - acc: 0.5108\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 126s 5ms/step - loss: 0.6285 - acc: 0.6402\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 131s 5ms/step - loss: 0.4731 - acc: 0.7811\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 127s 5ms/step - loss: 0.4234 - acc: 0.8096\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 142s 6ms/step - loss: 0.4093 - acc: 0.8173\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 127s 5ms/step - loss: 0.4041 - acc: 0.8166\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 128s 5ms/step - loss: 0.3896 - acc: 0.8266\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 131s 5ms/step - loss: 0.3774 - acc: 0.8307\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 131s 5ms/step - loss: 0.3747 - acc: 0.8318\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 141s 6ms/step - loss: 0.3659 - acc: 0.8374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc1acec3828>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "\n",
    "model.fit(X_train, Y_train,batch_size=batch_size,epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 35s 1ms/step\n",
      "Accuracy on test set : 0.84036\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on test set : {}\".format(model.evaluate(X_test,Y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GRU\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 200, 50)           50000     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 100)               45300     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 250)               25250     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 120,801\n",
      "Trainable params: 120,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(n_words,50,input_length=max_len))\n",
    "model2.add(GRU(100,dropout=0.2,recurrent_dropout=0.2))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(250,activation=\"relu\"))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model2.compile(loss=\"binary_crossentropy\",optimizer=\"adagrad\",metrics=[\"accuracy\"])\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor=\"val_acc\",patience=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "20000/20000 [==============================] - 39s 2ms/step - loss: 0.2831 - acc: 0.8842 - val_loss: 0.2640 - val_acc: 0.8930\n",
      "Epoch 2/100\n",
      "20000/20000 [==============================] - 40s 2ms/step - loss: 0.2821 - acc: 0.8845 - val_loss: 0.2638 - val_acc: 0.8928\n",
      "Epoch 3/100\n",
      "20000/20000 [==============================] - 41s 2ms/step - loss: 0.2821 - acc: 0.8839 - val_loss: 0.2650 - val_acc: 0.8926\n",
      "Epoch 4/100\n",
      "20000/20000 [==============================] - 41s 2ms/step - loss: 0.2814 - acc: 0.8854 - val_loss: 0.2676 - val_acc: 0.8924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc19cda5080>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 512\n",
    "n_epochs = 100\n",
    "\n",
    "model2.fit(X_train, Y_train,batch_size=batch_size,epochs=n_epochs,validation_split=0.2,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 28s 1ms/step\n",
      "Accuracy on test set : 0.86796\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on test set : {}\".format(model2.evaluate(X_test,Y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-directional RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 200, 50)           50000     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               120800    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 250)               50250     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 221,301\n",
      "Trainable params: 221,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bi_model = Sequential()\n",
    "bi_model.add(Embedding(n_words,50,input_length=max_len))\n",
    "bi_model.add(Bidirectional(LSTM(100,dropout=0.2,recurrent_dropout=0.2)))\n",
    "bi_model.add(Dense(250,activation=\"relu\"))\n",
    "bi_model.add(Dropout(0.2))\n",
    "bi_model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "bi_model.summary()\n",
    "bi_model.compile(loss=\"binary_crossentropy\",optimizer=\"adagrad\",metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "20000/20000 [==============================] - 109s 5ms/step - loss: 0.7220 - acc: 0.6121 - val_loss: 0.5615 - val_acc: 0.7090\n",
      "Epoch 2/100\n",
      "20000/20000 [==============================] - 104s 5ms/step - loss: 0.5091 - acc: 0.7501 - val_loss: 0.4960 - val_acc: 0.7684\n",
      "Epoch 3/100\n",
      "20000/20000 [==============================] - 107s 5ms/step - loss: 0.4556 - acc: 0.7910 - val_loss: 0.4643 - val_acc: 0.7748\n",
      "Epoch 4/100\n",
      "20000/20000 [==============================] - 119s 6ms/step - loss: 0.4275 - acc: 0.8061 - val_loss: 0.4903 - val_acc: 0.7644\n",
      "Epoch 5/100\n",
      "20000/20000 [==============================] - 129s 6ms/step - loss: 0.4206 - acc: 0.8122 - val_loss: 0.5531 - val_acc: 0.7562\n",
      "Epoch 6/100\n",
      "20000/20000 [==============================] - 115s 6ms/step - loss: 0.4131 - acc: 0.8183 - val_loss: 0.4781 - val_acc: 0.7914\n",
      "Epoch 7/100\n",
      "20000/20000 [==============================] - 109s 5ms/step - loss: 0.3968 - acc: 0.8279 - val_loss: 0.4184 - val_acc: 0.8084\n",
      "Epoch 8/100\n",
      "20000/20000 [==============================] - 110s 5ms/step - loss: 0.3856 - acc: 0.8361 - val_loss: 0.4497 - val_acc: 0.7892\n",
      "Epoch 9/100\n",
      "20000/20000 [==============================] - 109s 5ms/step - loss: 0.3786 - acc: 0.8382 - val_loss: 0.3894 - val_acc: 0.8260\n",
      "Epoch 10/100\n",
      "20000/20000 [==============================] - 108s 5ms/step - loss: 0.3635 - acc: 0.8459 - val_loss: 0.4258 - val_acc: 0.8162\n",
      "Epoch 11/100\n",
      "20000/20000 [==============================] - 113s 6ms/step - loss: 0.3682 - acc: 0.8461 - val_loss: 0.4123 - val_acc: 0.8138\n",
      "Epoch 12/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 0.3551 - acc: 0.8500 - val_loss: 0.4102 - val_acc: 0.8276\n",
      "Epoch 13/100\n",
      "20000/20000 [==============================] - 110s 6ms/step - loss: 0.3611 - acc: 0.8498 - val_loss: 0.3631 - val_acc: 0.8470\n",
      "Epoch 14/100\n",
      "20000/20000 [==============================] - 110s 6ms/step - loss: 0.3560 - acc: 0.8510 - val_loss: 0.4411 - val_acc: 0.8056\n",
      "Epoch 15/100\n",
      "20000/20000 [==============================] - 110s 5ms/step - loss: 0.3547 - acc: 0.8487 - val_loss: 0.4195 - val_acc: 0.8172\n",
      "Epoch 16/100\n",
      "20000/20000 [==============================] - 110s 6ms/step - loss: 0.3606 - acc: 0.8478 - val_loss: 0.3754 - val_acc: 0.8442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc19e45cef0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_model.fit(X_train,Y_train,batch_size=batch_size,epochs=n_epochs,validation_split=0.2,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 62s 2ms/step\n",
      "Accuracy on test set : 0.84464\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on test set : {}\".format(bi_model.evaluate(X_test,Y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
